{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Hospital Readmission Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the previous notebook describing the statistical analysis on Python, we will reproduce this study on Spark.\n",
    "Spark (PySpark) is the current hot datascience framework. It make easy working with very large dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should define the SPARK HOME PATH toward our local Spark built version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os,sys\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/lib/spark-1.6.1-bin-hadoop2.6\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every spark script should be started by defining a spark context. The context defines an application and should be configured with a a SparkConf. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is focusing on pySpark DataFrame and Machine learning (MLlib). Before dealing with DataFrame we have to load an useful library spark-csv from Databricks in order for parsing and querying CSV.\n",
    "Please, download library and uncompressin your spark root folder. https://github.com/databricks/spark-csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add to your notebook environnement the path to spark-csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-csv_2.10:1.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set configuration properties by passing a SparkConf object to SparkContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"1000\"\n",
       "            height=\"350\"\n",
       "            src=\"https://spark.apache.org/docs/0.9.1/python-programming-guide.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0xb30e2a2c>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://spark.apache.org/docs/0.9.1/python-programming-guide.html', width=1000, height=350)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark Context is the object through which we bridge our programs with the computing cluster that Spark provides. It's available in our environment as the variable sc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    "         .setMaster(\"local\")\n",
    "         .setAppName(\"HopitalReadmission\")\n",
    "         .set(\"spark.executor.memory\", \"1g\"))\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we create a new RDD and load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101767\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "fileName = \"/home/charles-abner/Documents/dataset_diabetes/diabetic_data.csv\"\n",
    "rawData = sc.textFile(fileName)\n",
    "header = rawData.first() #extract header\n",
    "print rawData.count()\n",
    "rawData = rawData.filter(lambda x:x !=header)    #filter out header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from pyspark.sql import DataFrameReader \n",
    "#df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Spark DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R or Pandas. They can be constructed from a wide array of sources such as an existing RDD in our case.\n",
    "\n",
    "The entry point into all SQL functionality in Spark is the SQLContext class. To create a basic instance, all we need is a SparkContext reference. Since we are running Spark in shell mode (using pySpark) we can use the global context object sc for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entry point into all functionality in Spark SQL is the SQLContext class, or one of its descendants. To create a basic SQLContext, all you need is a SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# sc is an existing SparkContext.\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now load data in a spark dataframe. Moreover, Spark dataframe offers best performances than RDDs and it is easier to manipulate dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, method to create a DataFrame and inferring variable types consists in using the DataBricks library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('encounter_id', 'int'),\n",
       " ('patient_nbr', 'int'),\n",
       " ('race', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('age', 'string'),\n",
       " ('weight', 'string'),\n",
       " ('admission_type_id', 'int'),\n",
       " ('discharge_disposition_id', 'int'),\n",
       " ('admission_source_id', 'int'),\n",
       " ('time_in_hospital', 'int'),\n",
       " ('payer_code', 'string'),\n",
       " ('medical_specialty', 'string'),\n",
       " ('num_lab_procedures', 'int'),\n",
       " ('num_procedures', 'int'),\n",
       " ('num_medications', 'int'),\n",
       " ('number_outpatient', 'int'),\n",
       " ('number_emergency', 'int'),\n",
       " ('number_inpatient', 'int'),\n",
       " ('diag_1', 'string'),\n",
       " ('diag_2', 'string'),\n",
       " ('diag_3', 'string'),\n",
       " ('number_diagnoses', 'int'),\n",
       " ('max_glu_serum', 'string'),\n",
       " ('A1Cresult', 'string'),\n",
       " ('metformin', 'string'),\n",
       " ('repaglinide', 'string'),\n",
       " ('nateglinide', 'string'),\n",
       " ('chlorpropamide', 'string'),\n",
       " ('glimepiride', 'string'),\n",
       " ('acetohexamide', 'string'),\n",
       " ('glipizide', 'string'),\n",
       " ('glyburide', 'string'),\n",
       " ('tolbutamide', 'string'),\n",
       " ('pioglitazone', 'string'),\n",
       " ('rosiglitazone', 'string'),\n",
       " ('acarbose', 'string'),\n",
       " ('miglitol', 'string'),\n",
       " ('troglitazone', 'string'),\n",
       " ('tolazamide', 'string'),\n",
       " ('examide', 'string'),\n",
       " ('citoglipton', 'string'),\n",
       " ('insulin', 'string'),\n",
       " ('glyburide-metformin', 'string'),\n",
       " ('glipizide-metformin', 'string'),\n",
       " ('glimepiride-pioglitazone', 'string'),\n",
       " ('metformin-rosiglitazone', 'string'),\n",
       " ('metformin-pioglitazone', 'string'),\n",
       " ('change', 'string'),\n",
       " ('diabetesMed', 'string'),\n",
       " ('readmitted', 'string')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_databricks = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load(fileName)\n",
    "df_from_databricks.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that method 'inferSchema' works well. You would change somee variable format by using the method  .cast()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second method to create a DataFrame consists in creating a DataFrame from a RDD by using a sqlContext. This method doesnt make possible inferring format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_data = rawData.map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the header and first row in a pandas dataframe to get information about format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_patient = csv_data.map(lambda p: Row(\n",
    "    encounter_id=p[0],\n",
    "    patient_nbr=p[1],\n",
    "    race =p[2],\n",
    "    gender=p[3],\n",
    "    age  =p[4],\n",
    "    weight =p[5],\n",
    "    admission_type_id =p[6],\n",
    "    discharge_disposition_id =p[7],\n",
    "    admission_source_id =p[8],\n",
    "    time_in_hospital =p[9],\n",
    "    payer_code =p[10],\n",
    "    medical_specialty =p[11],\n",
    "    num_lab_procedures =p[12],\n",
    "    num_procedures =p[13],\n",
    "    num_medications =p[14],\n",
    "    number_outpatient =p[15],\n",
    "    number_emergency =p[16],\n",
    "    number_inpatient =p[17],\n",
    "    diag_1   =p[18],\n",
    "    diag_2 =p[19],\n",
    "    diag_3 =p[20],\n",
    "    number_diagnoses  =int(p[21]),\n",
    "    max_glu_serum   =p[22],\n",
    "    A1Cresult       =p[23],\n",
    "    metformin       =p[24],\n",
    "    repaglinide     =p[25],\n",
    "    nateglinide     =p[26],                  \n",
    "    chlorpropamide  =p[27],               \n",
    "    glimepiride     =p[28],                  \n",
    "    acetohexamide   =p[29],                \n",
    "    glipizide       =p[30],                    \n",
    "    glyburide       =p[31],                    \n",
    "    tolbutamide     =p[32],                  \n",
    "    pioglitazone    =p[33],                 \n",
    "    rosiglitazone   =p[34],                \n",
    "    acarbose        =p[35],                     \n",
    "    miglitol        =p[36],                     \n",
    "    troglitazone    =p[37],                 \n",
    "    tolazamide      =p[38],                   \n",
    "    examide         =p[39],                            \n",
    "    citoglipton     =p[40],   \n",
    "    insulin         =p[41],   \n",
    "    glyburide_metformin  =p[42],   \n",
    "    glipizide_metformin  =p[43],   \n",
    "    glimepiride_pioglitazone  =p[44],   \n",
    "    metformin_rosiglitazone   =p[45],   \n",
    "    metformin_pioglitazone    =p[46],   \n",
    "    change     =p[47],   \n",
    "    diabetesMed=p[48],   \n",
    "    readmitted  =p[49]           \n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_from_rdd = sqlContext.createDataFrame(row_patient)\n",
    "df_from_rdd.registerTempTable(\"people\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get easily get the data schema. Actually, Spark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys of this list define the column names of the table, and the types are inferred by looking at the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- A1Cresult: string (nullable = true)\n",
      " |-- acarbose: string (nullable = true)\n",
      " |-- acetohexamide: string (nullable = true)\n",
      " |-- admission_source_id: string (nullable = true)\n",
      " |-- admission_type_id: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- change: string (nullable = true)\n",
      " |-- chlorpropamide: string (nullable = true)\n",
      " |-- citoglipton: string (nullable = true)\n",
      " |-- diabetesMed: string (nullable = true)\n",
      " |-- diag_1: string (nullable = true)\n",
      " |-- diag_2: string (nullable = true)\n",
      " |-- diag_3: string (nullable = true)\n",
      " |-- discharge_disposition_id: string (nullable = true)\n",
      " |-- encounter_id: string (nullable = true)\n",
      " |-- examide: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- glimepiride: string (nullable = true)\n",
      " |-- glimepiride_pioglitazone: string (nullable = true)\n",
      " |-- glipizide: string (nullable = true)\n",
      " |-- glipizide_metformin: string (nullable = true)\n",
      " |-- glyburide: string (nullable = true)\n",
      " |-- glyburide_metformin: string (nullable = true)\n",
      " |-- insulin: string (nullable = true)\n",
      " |-- max_glu_serum: string (nullable = true)\n",
      " |-- medical_specialty: string (nullable = true)\n",
      " |-- metformin: string (nullable = true)\n",
      " |-- metformin_pioglitazone: string (nullable = true)\n",
      " |-- metformin_rosiglitazone: string (nullable = true)\n",
      " |-- miglitol: string (nullable = true)\n",
      " |-- nateglinide: string (nullable = true)\n",
      " |-- num_lab_procedures: string (nullable = true)\n",
      " |-- num_medications: string (nullable = true)\n",
      " |-- num_procedures: string (nullable = true)\n",
      " |-- number_diagnoses: long (nullable = true)\n",
      " |-- number_emergency: string (nullable = true)\n",
      " |-- number_inpatient: string (nullable = true)\n",
      " |-- number_outpatient: string (nullable = true)\n",
      " |-- patient_nbr: string (nullable = true)\n",
      " |-- payer_code: string (nullable = true)\n",
      " |-- pioglitazone: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- readmitted: string (nullable = true)\n",
      " |-- repaglinide: string (nullable = true)\n",
      " |-- rosiglitazone: string (nullable = true)\n",
      " |-- time_in_hospital: string (nullable = true)\n",
      " |-- tolazamide: string (nullable = true)\n",
      " |-- tolbutamide: string (nullable = true)\n",
      " |-- troglitazone: string (nullable = true)\n",
      " |-- weight: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_rdd.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Queries as DataFrame operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark DataFrame provides a domain-specific language for structured data manipulation. This language includes methods we can concatenate in order to do selection, filtering, grouping, etc. For example, let's say we want to count how many people are there for each age interval. We can proceed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|     age|count|\n",
      "+--------+-----+\n",
      "| [70-80)|26068|\n",
      "|[90-100)| 2793|\n",
      "| [20-30)| 1657|\n",
      "| [40-50)| 9685|\n",
      "| [60-70)|22483|\n",
      "|  [0-10)|  161|\n",
      "| [80-90)|17197|\n",
      "| [10-20)|  691|\n",
      "| [30-40)| 3775|\n",
      "| [50-60)|17256|\n",
      "+--------+-----+\n",
      "\n",
      "Query performed in 3.002 seconds\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "t0 = time()\n",
    "df_from_databricks.select(\"admission_type_id\", \"weight\", \"gender\", \"age\").groupBy(\"age\").count().show()\n",
    "tt = time() - t0\n",
    "\n",
    "print \"Query performed in {} seconds\".format(round(tt,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine we would parse the age interval to get the decade. Let's parse age to keep the second caracter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDecade(age):\n",
    "    return int(age[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "row_age_parsed = csv_data.map(lambda p: Row(\n",
    "    encounter_id=p[0],\n",
    "    patient_nbr=p[1],\n",
    "    race =p[2],\n",
    "    gender=p[3],\n",
    "    age  =p[4],\n",
    "    age_decade  =getDecade(p[4]),\n",
    "    age_string  =p[4],\n",
    "    weight =p[5],\n",
    "    admission_type_id =p[6],\n",
    "    discharge_disposition_id =p[7],\n",
    "    admission_source_id =p[8],\n",
    "    time_in_hospital =p[9],\n",
    "    payer_code =p[10],\n",
    "    medical_specialty =p[11],\n",
    "    num_lab_procedures =p[12],\n",
    "    num_procedures =p[13],\n",
    "    num_medications =p[14],\n",
    "    number_outpatient =p[15],\n",
    "    number_emergency =p[16],\n",
    "    number_inpatient =p[17],\n",
    "    diag_1   =p[18],\n",
    "    diag_2 =p[19],\n",
    "    diag_3 =p[20],\n",
    "    number_diagnoses  =int(p[21]),\n",
    "    max_glu_serum   =p[22],\n",
    "    A1Cresult       =p[23],\n",
    "    metformin       =p[24],\n",
    "    repaglinide     =p[25],\n",
    "    nateglinide     =p[26],                  \n",
    "    chlorpropamide  =p[27],               \n",
    "    glimepiride     =p[28],                  \n",
    "    acetohexamide   =p[29],                \n",
    "    glipizide       =p[30],                    \n",
    "    glyburide       =p[31],                    \n",
    "    tolbutamide     =p[32],                  \n",
    "    pioglitazone    =p[33],                 \n",
    "    rosiglitazone   =p[34],                \n",
    "    acarbose        =p[35],                     \n",
    "    miglitol        =p[36],                     \n",
    "    troglitazone    =p[37],                 \n",
    "    tolazamide      =p[38],                   \n",
    "    examide         =p[39],                            \n",
    "    citoglipton     =p[40],   \n",
    "    insulin         =p[41],   \n",
    "    glyburide_metformin  =p[42],   \n",
    "    glipizide_metformin  =p[43],   \n",
    "    glimepiride_pioglitazone  =p[44],   \n",
    "    metformin_rosiglitazone   =p[45],   \n",
    "    metformin_pioglitazone    =p[46],   \n",
    "    change     =p[47],   \n",
    "    diabetesMed=p[48],   \n",
    "    readmitted  =p[49]            \n",
    "    )\n",
    ")\n",
    "#finally we store data in a new dataframe\n",
    "df_from_rdd_enriched = sqlContext.createDataFrame(row_age_parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, with the df_from_databricks we can create new variables as follow :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with variable transformation. We would often compute a new variable resulting from a transformation on already existing columns.  Contrary to pandas you could not use df[newVariable]=f(df[existingVariable]). You have to use a pySpark method call withColumns : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<cast(age_string as string)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we convert age in string\n",
    "df_from_databricks = df_from_databricks.withColumn(\"age_string\", df_from_databricks[\"age\"])\n",
    "df_from_databricks.age_string.cast('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "getDecadeUdf = udf(getDecade, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|age_decade|\n",
      "+----------+\n",
      "|         0|\n",
      "|         1|\n",
      "|         2|\n",
      "|         3|\n",
      "|         4|\n",
      "|         5|\n",
      "|         6|\n",
      "|         7|\n",
      "|         8|\n",
      "|         9|\n",
      "|         4|\n",
      "|         6|\n",
      "|         4|\n",
      "|         8|\n",
      "|         6|\n",
      "|         6|\n",
      "|         5|\n",
      "|         5|\n",
      "|         7|\n",
      "|         7|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let apply the udf to extract the decade from age\n",
    "df_from_databricks = df_from_databricks.withColumn(\"age_decade\", getDecadeUdf(df_from_databricks[\"age\"]))\n",
    "df_from_databricks.select(\"age_decade\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age  gender age_string  count\n",
      "0   [70-80)    Male    [70-80)  12081\n",
      "1   [80-90)  Female    [80-90)  10515\n",
      "2   [60-70)    Male    [60-70)  11421\n",
      "3  [90-100)  Female   [90-100)   2003\n",
      "4   [70-80)  Female    [70-80)  13985\n"
     ]
    }
   ],
   "source": [
    "distAge = df_from_databricks.select('encounter_id','gender','age','age_string').groupBy(\"age\",'gender',\"age_string\").count()\n",
    "distAge_pd = distAge.toPandas()\n",
    "print distAge_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---------------+-----+\n",
      "|age_gender|Female|Unknown/Invalid| Male|\n",
      "+----------+------+---------------+-----+\n",
      "|   [50-60)|  8572|              0| 8684|\n",
      "|   [70-80)| 13985|              2|12081|\n",
      "|  [90-100)|  2003|              0|  790|\n",
      "|   [40-50)|  4811|              0| 4874|\n",
      "|    [0-10)|    83|              0|   78|\n",
      "|   [20-30)|  1114|              0|  543|\n",
      "|   [30-40)|  2162|              0| 1613|\n",
      "|   [60-70)| 11061|              1|11421|\n",
      "|   [80-90)| 10515|              0| 6682|\n",
      "|   [10-20)|   402|              0|  289|\n",
      "+----------+------+---------------+-----+\n",
      "\n",
      "None\n",
      "+-----------------+----+-----+-----+\n",
      "|gender_readmitted| <30|   NO|  >30|\n",
      "+-----------------+----+-----+-----+\n",
      "|  Unknown/Invalid|   0|    3|    0|\n",
      "|             Male|5205|25823|16027|\n",
      "|           Female|6152|29038|19518|\n",
      "+-----------------+----+-----+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Another useful function is crosstable which compute a Cross Tabulation (Contingency Table)\n",
    "#Cross Tabulation provides a table of the frequency distribution for a set of variables. \n",
    "print df_from_databricks.stat.crosstab(\"age\", \"gender\").show()\n",
    "print df_from_databricks.stat.crosstab(\"gender\", \"readmitted\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrGIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xad634a8c>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "age = distAge_pd['age_string']\n",
    "countMen = distAge_pd.loc[distAge_pd['gender'] == \"Male\"][\"count\"]\n",
    "countWomen = distAge_pd.loc[distAge_pd['gender'] == \"Female\"][\"count\"]\n",
    "N = 10\n",
    "ind = np.arange(N) # the x locations for the groups\n",
    "width = 0.2       # the width of the bars: can also be len(x) sequence\n",
    "\n",
    "ax = plt.subplot(111)\n",
    "p1 = plt.bar(ind, countMen, width, color='b')\n",
    "p2 = plt.bar(ind, countWomen, width, color='r')\n",
    "\n",
    "plt.ylabel('People distribution')\n",
    "plt.title('Count people by age and gender')\n",
    "plt.xticks(ind + width/2., age)\n",
    "plt.yticks(np.arange(0,20000, 5000))\n",
    "plt.legend((p1[0], p2[0]), ('Men', 'Women'))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Spark Transformation and Action for feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before processing and transform some variable we would be interested by a general data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, age_decade: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_databricks[[\"age_decade\"]].describe()\n",
    "# you can note some bug. Actually, age is bigint but recognized by .describe() as string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering is pretty much straightforward too, you can use the *RDD-like* filter method and copy any of your existing Pandas expression/predicate for filtering :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 101766 patients \n",
      "There are 66221 patients with readmission value equals to '<30' or 'No'\n"
     ]
    }
   ],
   "source": [
    "# use select to show a particular column\n",
    "#df.select('readmitted').show()\n",
    "#we would drop row with readmitted with value \">30\" to have a two-classes classification.\n",
    "print \"There are %s patients \" % df_from_databricks.count()\n",
    "df_filtered = df_from_databricks.filter(df_from_databricks.readmitted !=\">30\").count()\n",
    "print \"There are %s patients with readmission value equals to '<30' or 'No'\" %  df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admission_type_id', 'discharge_disposition_id', 'admission_source_id', 'time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_outpatient', 'number_emergency', 'number_inpatient', 'number_diagnoses', 'age_decade']\n"
     ]
    }
   ],
   "source": [
    "#We would keep only numeric column to compute correlation 2-2\n",
    "colInt = [ c[0] for c in df_from_databricks.dtypes if ((c[1] in ['bigint','int','float']) and (c[0] not in ['encounter_id','patient_nbr']))]\n",
    "print colInt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics with mllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now introducing Spark's machine learning library MLlib through its basic statistics functionality in order to better understand our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Descriptive and exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark.sql already contains some basic statistical function but we will focus on mllib.Statistics which provides methods to calculate correlations between series. Firstly, we would compute a correlation matrix from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04519734115117919"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import DataFrameStatFunctions\n",
    "df_from_databricks.corr('num_medications','number_outpatient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.mllib.stat import MultivariateStatisticalSummary\n",
    "from pyspark.mllib.linalg import Vector\n",
    "from math import sqrt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0451973411512\n"
     ]
    }
   ],
   "source": [
    "seriesX = df_from_databricks.select('num_medications').rdd.map(lambda x:float(x[0]))\n",
    "seriesY = df_from_databricks.select('number_outpatient').rdd.map(lambda x:float(x[0]))\n",
    "print(Statistics.corr(seriesX, seriesY, method=\"pearson\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_from_df = df_from_databricks.select(colInt).rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4343.corr.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 1930, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 77, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <type 'float'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1328)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1302)\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1342)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1341)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:61)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:328)\n\tat org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationMatrix(PearsonCorrelation.scala:49)\n\tat org.apache.spark.mllib.stat.correlation.Correlations$.corrMatrix(Correlation.scala:66)\n\tat org.apache.spark.mllib.stat.Statistics$.corr(Statistics.scala:74)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.corr(PythonMLLibAPI.scala:813)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 77, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <type 'float'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-205-89b6ee0385a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd_from_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStatistics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pearson\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/lib/spark/python/pyspark/mllib/stat/_statistics.py\u001b[0m in \u001b[0;36mcorr\u001b[1;34m(x, y, method)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"corr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_to_vector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mcallMLlibFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"corr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallMLlibFunc\u001b[1;34m(name, *args)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mapi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonMLLibAPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcallJavaFunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/pyspark/mllib/common.py\u001b[0m in \u001b[0;36mcallJavaFunc\u001b[1;34m(sc, func, *args)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;34m\"\"\" Call Java Function \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_py2java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_java2py\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 933\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/spark/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/py4j/protocol.pyc\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o4343.corr.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 155.0 failed 1 times, most recent failure: Lost task 0.0 in stage 155.0 (TID 1930, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 77, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <type 'float'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1328)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1302)\n\tat org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1342)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1341)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.numCols(RowMatrix.scala:61)\n\tat org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:328)\n\tat org.apache.spark.mllib.stat.correlation.PearsonCorrelation$.computeCorrelationMatrix(PearsonCorrelation.scala:49)\n\tat org.apache.spark.mllib.stat.correlation.Correlations$.corrMatrix(Correlation.scala:66)\n\tat org.apache.spark.mllib.stat.Statistics$.corr(Statistics.scala:74)\n\tat org.apache.spark.mllib.api.python.PythonMLLibAPI.corr(PythonMLLibAPI.scala:813)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 111, in main\n    process()\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/worker.py\", line 106, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/usr/lib/spark-1.6.1-bin-hadoop2.6/python/lib/pyspark.zip/pyspark/mllib/linalg/__init__.py\", line 77, in _convert_to_vector\n    raise TypeError(\"Cannot convert type %s into Vector\" % type(l))\nTypeError: Cannot convert type <type 'float'> into Vector\n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:207)\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:125)\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "data = rdd_from_df.map(lambda r:float(r[0]))\n",
    "print(Statistics.corr(data, method=\"pearson\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0451973411512\n"
     ]
    }
   ],
   "source": [
    "print(Statistics.corr(seriesX, seriesY, method=\"pearson\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[encounter_id: int, patient_nbr: int, race: string, gender: string, age: string, weight: string, admission_type_id: int, discharge_disposition_id: int, admission_source_id: int, time_in_hospital: int, payer_code: string, medical_specialty: string, num_lab_procedures: int, num_procedures: int, num_medications: int, number_outpatient: int, number_emergency: int, number_inpatient: int, diag_1: string, diag_2: string, diag_3: string, number_diagnoses: int, max_glu_serum: string, A1Cresult: string, metformin: string, repaglinide: string, nateglinide: string, chlorpropamide: string, glimepiride: string, acetohexamide: string, glipizide: string, glyburide: string, tolbutamide: string, pioglitazone: string, rosiglitazone: string, acarbose: string, miglitol: string, troglitazone: string, tolazamide: string, examide: string, citoglipton: string, insulin: string, glyburide-metformin: string, glipizide-metformin: string, glimepiride-pioglitazone: string, metformin-rosiglitazone: string, metformin-pioglitazone: string, change: string, diabetesMed: string, age_string: string, age_decade: int]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_databricks[[c for c in df_from_databricks.columns if c not in [\"readmitted\"]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(encounter_id=2278392, patient_nbr=8222157, race=u'Caucasian', gender=u'Female', age=u'[0-10)', weight=u'?', admission_type_id=6, discharge_disposition_id=25, admission_source_id=1, time_in_hospital=1, payer_code=u'?', medical_specialty=u'Pediatrics-Endocrinology', num_lab_procedures=41, num_procedures=0, num_medications=1, number_outpatient=0, number_emergency=0, number_inpatient=0, diag_1=u'250.83', diag_2=u'?', diag_3=u'?', number_diagnoses=1, max_glu_serum=u'None', A1Cresult=u'None', metformin=u'No', repaglinide=u'No', nateglinide=u'No', chlorpropamide=u'No', glimepiride=u'No', acetohexamide=u'No', glipizide=u'No', glyburide=u'No', tolbutamide=u'No', pioglitazone=u'No', rosiglitazone=u'No', acarbose=u'No', miglitol=u'No', troglitazone=u'No', tolazamide=u'No', examide=u'No', citoglipton=u'No', insulin=u'No', glyburide-metformin=u'No', glipizide-metformin=u'No', glimepiride-pioglitazone=u'No', metformin-rosiglitazone=u'No', metformin-pioglitazone=u'No', change=u'No', diabetesMed=u'No', readmitted=u'NO', age_string=u'[0-10)', age_decade=0),\n",
       " Row(encounter_id=149190, patient_nbr=55629189, race=u'Caucasian', gender=u'Female', age=u'[10-20)', weight=u'?', admission_type_id=1, discharge_disposition_id=1, admission_source_id=7, time_in_hospital=3, payer_code=u'?', medical_specialty=u'?', num_lab_procedures=59, num_procedures=0, num_medications=18, number_outpatient=0, number_emergency=0, number_inpatient=0, diag_1=u'276', diag_2=u'250.01', diag_3=u'255', number_diagnoses=9, max_glu_serum=u'None', A1Cresult=u'None', metformin=u'No', repaglinide=u'No', nateglinide=u'No', chlorpropamide=u'No', glimepiride=u'No', acetohexamide=u'No', glipizide=u'No', glyburide=u'No', tolbutamide=u'No', pioglitazone=u'No', rosiglitazone=u'No', acarbose=u'No', miglitol=u'No', troglitazone=u'No', tolazamide=u'No', examide=u'No', citoglipton=u'No', insulin=u'Up', glyburide-metformin=u'No', glipizide-metformin=u'No', glimepiride-pioglitazone=u'No', metformin-rosiglitazone=u'No', metformin-pioglitazone=u'No', change=u'Ch', diabetesMed=u'Yes', readmitted=u'>30', age_string=u'[10-20)', age_decade=1),\n",
       " Row(encounter_id=64410, patient_nbr=86047875, race=u'AfricanAmerican', gender=u'Female', age=u'[20-30)', weight=u'?', admission_type_id=1, discharge_disposition_id=1, admission_source_id=7, time_in_hospital=2, payer_code=u'?', medical_specialty=u'?', num_lab_procedures=11, num_procedures=5, num_medications=13, number_outpatient=2, number_emergency=0, number_inpatient=1, diag_1=u'648', diag_2=u'250', diag_3=u'V27', number_diagnoses=6, max_glu_serum=u'None', A1Cresult=u'None', metformin=u'No', repaglinide=u'No', nateglinide=u'No', chlorpropamide=u'No', glimepiride=u'No', acetohexamide=u'No', glipizide=u'Steady', glyburide=u'No', tolbutamide=u'No', pioglitazone=u'No', rosiglitazone=u'No', acarbose=u'No', miglitol=u'No', troglitazone=u'No', tolazamide=u'No', examide=u'No', citoglipton=u'No', insulin=u'No', glyburide-metformin=u'No', glipizide-metformin=u'No', glimepiride-pioglitazone=u'No', metformin-rosiglitazone=u'No', metformin-pioglitazone=u'No', change=u'No', diabetesMed=u'Yes', readmitted=u'NO', age_string=u'[20-30)', age_decade=2),\n",
       " Row(encounter_id=500364, patient_nbr=82442376, race=u'Caucasian', gender=u'Male', age=u'[30-40)', weight=u'?', admission_type_id=1, discharge_disposition_id=1, admission_source_id=7, time_in_hospital=2, payer_code=u'?', medical_specialty=u'?', num_lab_procedures=44, num_procedures=1, num_medications=16, number_outpatient=0, number_emergency=0, number_inpatient=0, diag_1=u'8', diag_2=u'250.43', diag_3=u'403', number_diagnoses=7, max_glu_serum=u'None', A1Cresult=u'None', metformin=u'No', repaglinide=u'No', nateglinide=u'No', chlorpropamide=u'No', glimepiride=u'No', acetohexamide=u'No', glipizide=u'No', glyburide=u'No', tolbutamide=u'No', pioglitazone=u'No', rosiglitazone=u'No', acarbose=u'No', miglitol=u'No', troglitazone=u'No', tolazamide=u'No', examide=u'No', citoglipton=u'No', insulin=u'Up', glyburide-metformin=u'No', glipizide-metformin=u'No', glimepiride-pioglitazone=u'No', metformin-rosiglitazone=u'No', metformin-pioglitazone=u'No', change=u'Ch', diabetesMed=u'Yes', readmitted=u'NO', age_string=u'[30-40)', age_decade=3),\n",
       " Row(encounter_id=16680, patient_nbr=42519267, race=u'Caucasian', gender=u'Male', age=u'[40-50)', weight=u'?', admission_type_id=1, discharge_disposition_id=1, admission_source_id=7, time_in_hospital=1, payer_code=u'?', medical_specialty=u'?', num_lab_procedures=51, num_procedures=0, num_medications=8, number_outpatient=0, number_emergency=0, number_inpatient=0, diag_1=u'197', diag_2=u'157', diag_3=u'250', number_diagnoses=5, max_glu_serum=u'None', A1Cresult=u'None', metformin=u'No', repaglinide=u'No', nateglinide=u'No', chlorpropamide=u'No', glimepiride=u'No', acetohexamide=u'No', glipizide=u'Steady', glyburide=u'No', tolbutamide=u'No', pioglitazone=u'No', rosiglitazone=u'No', acarbose=u'No', miglitol=u'No', troglitazone=u'No', tolazamide=u'No', examide=u'No', citoglipton=u'No', insulin=u'Steady', glyburide-metformin=u'No', glipizide-metformin=u'No', glimepiride-pioglitazone=u'No', metformin-rosiglitazone=u'No', metformin-pioglitazone=u'No', change=u'Ch', diabetesMed=u'Yes', readmitted=u'NO', age_string=u'[40-50)', age_decade=4),\n",
       " Row(encounter_id=35754, patient_nbr=82637451, race=u'Caucasian', gender=u'Male', age=u'[50-60)', weight=u'?', admission_type_id=2, discharge_disposition_id=1, admission_source_id=2, time_in_hospital=3, payer_code=u'?', medical_specialty=u'?', num_lab_procedures=31, num_procedures=6, num_medications=16, number_outpatient=0, number_emergency=0, number_inpatient=0, diag_1=u'414', diag_2=u'411', diag_3=u'250', number_diagnoses=9, max_glu_serum=u'None', A1Cresult=u'None', metformin=u'No', repaglinide=u'No', nateglinide=u'No', chlorpropamide=u'No', glimepiride=u'No', acetohexamide=u'No', glipizide=u'No', glyburide=u'No', tolbutamide=u'No', pioglitazone=u'No', rosiglitazone=u'No', acarbose=u'No', miglitol=u'No', troglitazone=u'No', tolazamide=u'No', examide=u'No', citoglipton=u'No', insulin=u'Steady', glyburide-metformin=u'No', glipizide-metformin=u'No', glimepiride-pioglitazone=u'No', metformin-rosiglitazone=u'No', metformin-pioglitazone=u'No', change=u'No', diabetesMed=u'Yes', readmitted=u'>30', age_string=u'[50-60)', age_decade=5),\n",
       " Row(encounter_id=55842, patient_nbr=84259809, race=u'Caucasian', gender=u'Male', age=u'[60-70)', weight=u'?', admission_type_id=3, discharge_disposition_id=1, admission_source_id=2, time_in_hospital=4, payer_code=u'?', medical_specialty=u'?', num_lab_procedures=70, num_procedures=1, num_medications=21, number_outpatient=0, number_emergency=0, number_inpatient=0, diag_1=u'414', diag_2=u'411', diag_3=u'V45', number_diagnoses=7, max_glu_serum=u'None', A1Cresult=u'None', metformin=u'Steady', repaglinide=u'No', nateglinide=u'No', chlorpropamide=u'No', glimepiride=u'Steady', acetohexamide=u'No', glipizide=u'No', glyburide=u'No', tolbutamide=u'No', pioglitazone=u'No', rosiglitazone=u'No', acarbose=u'No', miglitol=u'No', troglitazone=u'No', tolazamide=u'No', examide=u'No', citoglipton=u'No', insulin=u'Steady', glyburide-metformin=u'No', glipizide-metformin=u'No', glimepiride-pioglitazone=u'No', metformin-rosiglitazone=u'No', metformin-pioglitazone=u'No', change=u'Ch', diabetesMed=u'Yes', readmitted=u'NO', age_string=u'[60-70)', age_decade=6),\n",
       " Row(encounter_id=63768, patient_nbr=114882984, race=u'Caucasian', gender=u'Male', age=u'[70-80)', weight=u'?', admission_type_id=1, discharge_disposition_id=1, admission_source_id=7, time_in_hospital=5, payer_code=u'?', medical_specialty=u'?', num_lab_procedures=73, num_procedures=0, num_medications=12, number_outpatient=0, number_emergency=0, number_inpatient=0, diag_1=u'428', diag_2=u'492', diag_3=u'250', number_diagnoses=8, max_glu_serum=u'None', A1Cresult=u'None', metformin=u'No', repaglinide=u'No', nateglinide=u'No', chlorpropamide=u'No', glimepiride=u'No', acetohexamide=u'No', glipizide=u'No', glyburide=u'Steady', tolbutamide=u'No', pioglitazone=u'No', rosiglitazone=u'No', acarbose=u'No', miglitol=u'No', troglitazone=u'No', tolazamide=u'No', examide=u'No', citoglipton=u'No', insulin=u'No', glyburide-metformin=u'No', glipizide-metformin=u'No', glimepiride-pioglitazone=u'No', metformin-rosiglitazone=u'No', metformin-pioglitazone=u'No', change=u'No', diabetesMed=u'Yes', readmitted=u'>30', age_string=u'[70-80)', age_decade=7),\n",
       " Row(encounter_id=12522, patient_nbr=48330783, race=u'Caucasian', gender=u'Female', age=u'[80-90)', weight=u'?', admission_type_id=2, discharge_disposition_id=1, admission_source_id=4, time_in_hospital=13, payer_code=u'?', medical_specialty=u'?', num_lab_procedures=68, num_procedures=2, num_medications=28, number_outpatient=0, number_emergency=0, number_inpatient=0, diag_1=u'398', diag_2=u'427', diag_3=u'38', number_diagnoses=8, max_glu_serum=u'None', A1Cresult=u'None', metformin=u'No', repaglinide=u'No', nateglinide=u'No', chlorpropamide=u'No', glimepiride=u'No', acetohexamide=u'No', glipizide=u'Steady', glyburide=u'No', tolbutamide=u'No', pioglitazone=u'No', rosiglitazone=u'No', acarbose=u'No', miglitol=u'No', troglitazone=u'No', tolazamide=u'No', examide=u'No', citoglipton=u'No', insulin=u'Steady', glyburide-metformin=u'No', glipizide-metformin=u'No', glimepiride-pioglitazone=u'No', metformin-rosiglitazone=u'No', metformin-pioglitazone=u'No', change=u'Ch', diabetesMed=u'Yes', readmitted=u'NO', age_string=u'[80-90)', age_decade=8),\n",
       " Row(encounter_id=15738, patient_nbr=63555939, race=u'Caucasian', gender=u'Female', age=u'[90-100)', weight=u'?', admission_type_id=3, discharge_disposition_id=3, admission_source_id=4, time_in_hospital=12, payer_code=u'?', medical_specialty=u'InternalMedicine', num_lab_procedures=33, num_procedures=3, num_medications=18, number_outpatient=0, number_emergency=0, number_inpatient=0, diag_1=u'434', diag_2=u'198', diag_3=u'486', number_diagnoses=8, max_glu_serum=u'None', A1Cresult=u'None', metformin=u'No', repaglinide=u'No', nateglinide=u'No', chlorpropamide=u'No', glimepiride=u'No', acetohexamide=u'No', glipizide=u'No', glyburide=u'No', tolbutamide=u'No', pioglitazone=u'No', rosiglitazone=u'Steady', acarbose=u'No', miglitol=u'No', troglitazone=u'No', tolazamide=u'No', examide=u'No', citoglipton=u'No', insulin=u'Steady', glyburide-metformin=u'No', glipizide-metformin=u'No', glimepiride-pioglitazone=u'No', metformin-rosiglitazone=u'No', metformin-pioglitazone=u'No', change=u'Ch', diabetesMed=u'Yes', readmitted=u'NO', age_string=u'[90-100)', age_decade=9)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_databricks.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://databricks.com/blog/2014/03/26/spark-sql-manipulating-structured-data-using-spark-2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSQL Optimisation System called Catalyst \n",
    "\n",
    "https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
